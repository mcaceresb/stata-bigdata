{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"How to work efficiently with large datasets In Stata! This is a companion site to my presentation of the same name at the \"Empirical Research with Large Datasets\" workshop organized by the Banco de Portugal Microdata Research Laboratory (BPLIM). As I was making the slides I kept wanting to go into more detail and examples, so I decided to make this site for anyone interested in delving into the topics I touch upon in my talk in more depth. The outline of the site follows my presentation , but I may or may not augment this site over time with additional tips and tricks and ideas for working with big datasets in Stata.","title":"Home"},{"location":"index.html#how-to-work-efficiently-with-large-datasets","text":"In Stata! This is a companion site to my presentation of the same name at the \"Empirical Research with Large Datasets\" workshop organized by the Banco de Portugal Microdata Research Laboratory (BPLIM). As I was making the slides I kept wanting to go into more detail and examples, so I decided to make this site for anyone interested in delving into the topics I touch upon in my talk in more depth. The outline of the site follows my presentation , but I may or may not augment this site over time with additional tips and tricks and ideas for working with big datasets in Stata.","title":"How to work efficiently with large datasets"},{"location":"break.html","text":"What can break with big data? Generating IDs Consider a relatively common operation: Generating a unique identifier for each row of your data. Nothing too complicated: . clear . qui set obs `=2^24+5' . gen id = _n . format %21.0gc id . list in - 10 / l +------------+ | id | | ------------ | 16777212 . | 16 , 777 , 212 | 16777213 . | 16 , 777 , 213 | 16777214 . | 16 , 777 , 214 | 16777215 . | 16 , 777 , 215 | 16777216 . | 16 , 777 , 216 | | ------------ | 16777217 . | 16 , 777 , 216 | 16777218 . | 16 , 777 , 218 | 16777219 . | 16 , 777 , 220 | 16777220 . | 16 , 777 , 220 | 16777221 . | 16 , 777 , 220 | +------------+ . disp \" `:type id' \" float What went wrong? Stata's default type is float , a 4-byte floating point number. The largest integer it can accurately represent is 2 24 If we force it to represent larger integers, the variable overflows and we get repeated nubmers, as we can see. What can we do? You can can set the default type to double , an 8-byte floating point number which can represent integers up to 2 53 . Downside is increased memory use. The efficient solution is to use the c(obs_t) macro: gen `c(obs_t)' id = _n This uses the smallest integer type that can store the number of observations correctly. In general for any variable bounded above by the number of observations, this is a memory-efficient and safe solution (counts, IDs, etc.) Here you can see it in action: clear qui set obs 1 disp \" `c(obs_t)' \" // byte qui set obs `=maxbyte()+1' disp \" `c(obs_t)' \" // int qui set obs `=maxint()+1' disp \" `c(obs_t)' \" // long qui set obs `=maxlong()+1' disp \" `c(obs_t)' \" // double Numerical Precision This is a general issue when working with floating point numbers (i.e. real numbers, which are uncountably infinite, represented by finite data). However, big data can compound this problem. Large sums, for instance, can give incorrect results by some margin: . clear . set obs 1000000 . gen double x = sum (_n ^ 2 ) . gen double xsum = _n * (_n + 1 ) * ( 2 * _n + 1 ) / 6 . gen double xdif = x - xsum . sum xdif, d xdif ------------------------------------------------------------- Percentiles Smallest 1 % - 481312 - 490752 5 % - 443808 - 490752 10 % - 396928 - 490752 Obs 1 , 000 , 000 25 % - 256400 - 490720 Sum of Wgt. 1 , 000 , 000 50 % - 123600 Mean - 154691.7 Largest Std. Dev. 151885.8 75 % 0 . 5 90 % 0 . 5 Variance 2.31e+10 95 % 0 . 5 Skewness - . 5467313 99 % 0 . 5 Kurtosis 2.055727 You can see that for half the observations, the sum is off by more than 10 6 . If course, in relative terms this difference is negligible: . gen double xrel = reldif (x, xsum) . sum xrel, d xrel ------------------------------------------------------------- Percentiles Smallest 1 % 0 0 5 % 0 0 10 % 0 0 Obs 1 , 000 , 000 25 % 0 0 Sum of Wgt. 1 , 000 , 000 50 % 1.63e-12 Mean 1.51e-12 Largest Std. Dev. 1.20e-12 75 % 2.42e-12 3.80e-12 90 % 3.29e-12 3.80e-12 Variance 1.43e-24 95 % 3.63e-12 3.80e-12 Skewness . 1221362 99 % 3.78e-12 3.80e-12 Kurtosis 1.956908 This example merely underlines that care is required when working with large sums. (As a side-note, Stata's internal sums are often quadruple precision, meaning 16-byte floating point number, to avoid these types of issues.) A note on long long integers (64-bit, 8-bytes) Stata's largest integer type is long, 4 bytes as noted above. Sometimes, however, programs will use long long integers, which are 8 bytes. The longest integer represented by a double is 2 53 , but a long long integer can go up to 2 63 -1. Working around this limitation within Stata is possible since the addition of the Python interface (there's an interesting statalist thread about this where I pointed out such a solution). However, I will also say that sometimes Stata's not the best tool for the job, and stretching it to its limits is risky. I think it's only advisable to do so if using other software is prohibitive for some reason.","title":"What can break?"},{"location":"break.html#what-can-break-with-big-data","text":"","title":"What can break with big data?"},{"location":"break.html#generating-ids","text":"Consider a relatively common operation: Generating a unique identifier for each row of your data. Nothing too complicated: . clear . qui set obs `=2^24+5' . gen id = _n . format %21.0gc id . list in - 10 / l +------------+ | id | | ------------ | 16777212 . | 16 , 777 , 212 | 16777213 . | 16 , 777 , 213 | 16777214 . | 16 , 777 , 214 | 16777215 . | 16 , 777 , 215 | 16777216 . | 16 , 777 , 216 | | ------------ | 16777217 . | 16 , 777 , 216 | 16777218 . | 16 , 777 , 218 | 16777219 . | 16 , 777 , 220 | 16777220 . | 16 , 777 , 220 | 16777221 . | 16 , 777 , 220 | +------------+ . disp \" `:type id' \" float What went wrong? Stata's default type is float , a 4-byte floating point number. The largest integer it can accurately represent is 2 24 If we force it to represent larger integers, the variable overflows and we get repeated nubmers, as we can see. What can we do? You can can set the default type to double , an 8-byte floating point number which can represent integers up to 2 53 . Downside is increased memory use. The efficient solution is to use the c(obs_t) macro: gen `c(obs_t)' id = _n This uses the smallest integer type that can store the number of observations correctly. In general for any variable bounded above by the number of observations, this is a memory-efficient and safe solution (counts, IDs, etc.) Here you can see it in action: clear qui set obs 1 disp \" `c(obs_t)' \" // byte qui set obs `=maxbyte()+1' disp \" `c(obs_t)' \" // int qui set obs `=maxint()+1' disp \" `c(obs_t)' \" // long qui set obs `=maxlong()+1' disp \" `c(obs_t)' \" // double","title":"Generating IDs"},{"location":"break.html#numerical-precision","text":"This is a general issue when working with floating point numbers (i.e. real numbers, which are uncountably infinite, represented by finite data). However, big data can compound this problem. Large sums, for instance, can give incorrect results by some margin: . clear . set obs 1000000 . gen double x = sum (_n ^ 2 ) . gen double xsum = _n * (_n + 1 ) * ( 2 * _n + 1 ) / 6 . gen double xdif = x - xsum . sum xdif, d xdif ------------------------------------------------------------- Percentiles Smallest 1 % - 481312 - 490752 5 % - 443808 - 490752 10 % - 396928 - 490752 Obs 1 , 000 , 000 25 % - 256400 - 490720 Sum of Wgt. 1 , 000 , 000 50 % - 123600 Mean - 154691.7 Largest Std. Dev. 151885.8 75 % 0 . 5 90 % 0 . 5 Variance 2.31e+10 95 % 0 . 5 Skewness - . 5467313 99 % 0 . 5 Kurtosis 2.055727 You can see that for half the observations, the sum is off by more than 10 6 . If course, in relative terms this difference is negligible: . gen double xrel = reldif (x, xsum) . sum xrel, d xrel ------------------------------------------------------------- Percentiles Smallest 1 % 0 0 5 % 0 0 10 % 0 0 Obs 1 , 000 , 000 25 % 0 0 Sum of Wgt. 1 , 000 , 000 50 % 1.63e-12 Mean 1.51e-12 Largest Std. Dev. 1.20e-12 75 % 2.42e-12 3.80e-12 90 % 3.29e-12 3.80e-12 Variance 1.43e-24 95 % 3.63e-12 3.80e-12 Skewness . 1221362 99 % 3.78e-12 3.80e-12 Kurtosis 1.956908 This example merely underlines that care is required when working with large sums. (As a side-note, Stata's internal sums are often quadruple precision, meaning 16-byte floating point number, to avoid these types of issues.)","title":"Numerical Precision"},{"location":"break.html#a-note-on-long-long-integers-64-bit-8-bytes","text":"Stata's largest integer type is long, 4 bytes as noted above. Sometimes, however, programs will use long long integers, which are 8 bytes. The longest integer represented by a double is 2 53 , but a long long integer can go up to 2 63 -1. Working around this limitation within Stata is possible since the addition of the Python interface (there's an interesting statalist thread about this where I pointed out such a solution). However, I will also say that sometimes Stata's not the best tool for the job, and stretching it to its limits is risky. I think it's only advisable to do so if using other software is prohibitive for some reason.","title":"A note on long long integers (64-bit, 8-bytes)"},{"location":"tips.html","text":"This page is under construction; eventually it will compile in a list-like format all the tips, tricks, and suggestions in this website. Check back soon!","title":"Tips and Tricks/Summary"},{"location":"efficient/gtools.html","text":"Gtools There was a section in my presentation about gtools , which is the suite of commands I coded for working with big data more efficiently. While gtools still has to work within the contraints of Stata, it's underlying code is in C and is much faster than Stata in many places, to the point where some big data tasks are much less of a bottleneck. The original impetus for gtools was writing a faster collapse. While in Stata 17/MP, collapse has caught up to gcollapse , the package has expanded well beyond this original idea, and thare is a lot of functionality that remains much faster than any other Stata programs available. (Even in the case of collapse , gcollapse offers the merge and merge replace options, for example.) Please visit gtools.readthedocs.io for detailed documentation and examples. Below I reproduce some of the tables with an overview of how gtools compares to other Stata commands, which you can also find on the official site. Gtools commands with a Stata equivalent Function Replaces Speedup (IC / MP) Unsupported Extras gcollapse collapse -0.5 to 2 (Stata 17+); 4 to 100 (Stata 16 and earlier) Quantiles, merge, labels, nunique, etc. greshape reshape 4 to 20 / 4 to 15 \"advanced syntax\" fast , spread/gather (tidyr equiv) gegen egen 9 to 26 / 4 to 9 (+,.) labels Weights, quantiles, nunique, etc. gcontract contract 5 to 7 / 2.5 to 4 gisid isid 8 to 30 / 4 to 14 using , sort if , in glevelsof levelsof 3 to 13 / 2 to 7 Multiple variables, arbitrary levels gduplicates duplicates 8 to 16 / 3 to 10 gquantiles xtile 10 to 30 / 13 to 25 (-) by() , various (see usage ) pctile 13 to 38 / 3 to 5 (-) Ibid. _pctile 25 to 40 / 3 to 5 Ibid. gstats tab tabstat 10 to 50 / 5 to 30 (-) See remarks various (see usage ) gstats sum sum, detail 10 to 20 / 5 to 10 See remarks various (see usage ) (+) The upper end of the speed improvements are for quantiles (e.g. median, iqr, p90) and few groups. Weights have not been benchmarked. (.) Only gegen group was benchmarked rigorously. (-) Benchmarks computed 10 quantiles. When computing a large number of quantiles (e.g. thousands) pctile and xtile are prohibitively slow due to the way they are written; in that case gquantiles is hundreds or thousands of times faster, but this is an edge case. Extra commands Function Similar (SSC/SJ) Speedup (IC / MP) Notes fasterxtile fastxtile 20 to 30 / 2.5 to 3.5 Allows by() egenmisc (SSC) (-) 8 to 25 / 2.5 to 6 astile (SSC) (-) 8 to 12 / 3.5 to 6 gstats hdfe (.) Allows weights, by() gstats winsor winsor2 10 to 40 / 10 to 20 Allows weights gunique unique 4 to 26 / 4 to 12 gdistinct distinct 4 to 26 / 4 to 12 Also saves results in matrix gtop (gtoplevelsof) groups, select() (+) See table notes (+) gstats range rangestat 10 to 20 / 10 to 20 Allows weights; no flex stats gstats transform Various statistical functions (-) fastxtile from egenmisc and astile were benchmarked against gquantiles, xtile ( fasterxtile ) using by() . (+) While similar to the user command 'groups' with the 'select' option, gtoplevelsof does not really have an equivalent. It is several dozen times faster than 'groups, select', but that command was not written with the goal of gleaning the most common levels of a varlist. Rather, it has a plethora of features and that one is somewhat incidental. As such, the benchmark is not equivalent and gtoplevelsof does not attempt to implement the features of 'groups' (.) Other than the dated 'hdfe' command, I do not know of a stata command that residualizes variables from a set of fixed effects. The 'hdfe' command, as far as I can tell, morphed into the 'reghdfe' package; the latter, however, is a fully-functioning regression command, while 'gstats hdfe' only residualizes a set of variables.","title":"Gtools"},{"location":"efficient/gtools.html#gtools","text":"There was a section in my presentation about gtools , which is the suite of commands I coded for working with big data more efficiently. While gtools still has to work within the contraints of Stata, it's underlying code is in C and is much faster than Stata in many places, to the point where some big data tasks are much less of a bottleneck. The original impetus for gtools was writing a faster collapse. While in Stata 17/MP, collapse has caught up to gcollapse , the package has expanded well beyond this original idea, and thare is a lot of functionality that remains much faster than any other Stata programs available. (Even in the case of collapse , gcollapse offers the merge and merge replace options, for example.) Please visit gtools.readthedocs.io for detailed documentation and examples. Below I reproduce some of the tables with an overview of how gtools compares to other Stata commands, which you can also find on the official site. Gtools commands with a Stata equivalent Function Replaces Speedup (IC / MP) Unsupported Extras gcollapse collapse -0.5 to 2 (Stata 17+); 4 to 100 (Stata 16 and earlier) Quantiles, merge, labels, nunique, etc. greshape reshape 4 to 20 / 4 to 15 \"advanced syntax\" fast , spread/gather (tidyr equiv) gegen egen 9 to 26 / 4 to 9 (+,.) labels Weights, quantiles, nunique, etc. gcontract contract 5 to 7 / 2.5 to 4 gisid isid 8 to 30 / 4 to 14 using , sort if , in glevelsof levelsof 3 to 13 / 2 to 7 Multiple variables, arbitrary levels gduplicates duplicates 8 to 16 / 3 to 10 gquantiles xtile 10 to 30 / 13 to 25 (-) by() , various (see usage ) pctile 13 to 38 / 3 to 5 (-) Ibid. _pctile 25 to 40 / 3 to 5 Ibid. gstats tab tabstat 10 to 50 / 5 to 30 (-) See remarks various (see usage ) gstats sum sum, detail 10 to 20 / 5 to 10 See remarks various (see usage ) (+) The upper end of the speed improvements are for quantiles (e.g. median, iqr, p90) and few groups. Weights have not been benchmarked. (.) Only gegen group was benchmarked rigorously. (-) Benchmarks computed 10 quantiles. When computing a large number of quantiles (e.g. thousands) pctile and xtile are prohibitively slow due to the way they are written; in that case gquantiles is hundreds or thousands of times faster, but this is an edge case. Extra commands Function Similar (SSC/SJ) Speedup (IC / MP) Notes fasterxtile fastxtile 20 to 30 / 2.5 to 3.5 Allows by() egenmisc (SSC) (-) 8 to 25 / 2.5 to 6 astile (SSC) (-) 8 to 12 / 3.5 to 6 gstats hdfe (.) Allows weights, by() gstats winsor winsor2 10 to 40 / 10 to 20 Allows weights gunique unique 4 to 26 / 4 to 12 gdistinct distinct 4 to 26 / 4 to 12 Also saves results in matrix gtop (gtoplevelsof) groups, select() (+) See table notes (+) gstats range rangestat 10 to 20 / 10 to 20 Allows weights; no flex stats gstats transform Various statistical functions (-) fastxtile from egenmisc and astile were benchmarked against gquantiles, xtile ( fasterxtile ) using by() . (+) While similar to the user command 'groups' with the 'select' option, gtoplevelsof does not really have an equivalent. It is several dozen times faster than 'groups, select', but that command was not written with the goal of gleaning the most common levels of a varlist. Rather, it has a plethora of features and that one is somewhat incidental. As such, the benchmark is not equivalent and gtoplevelsof does not attempt to implement the features of 'groups' (.) Other than the dated 'hdfe' command, I do not know of a stata command that residualizes variables from a set of fixed effects. The 'hdfe' command, as far as I can tell, morphed into the 'reghdfe' package; the latter, however, is a fully-functioning regression command, while 'gstats hdfe' only residualizes a set of variables.","title":"Gtools"},{"location":"efficient/other-resources.html","text":"Don't reinvent the wheel! Someone may have already solved your problem. There are many user-written packages and programs that are designed to be efficient and may already do what you want to do. While custom tools tailored to your individual situation will typically be faster, it's often the case that the general-purpose tools are fast enough to justify avoiding making the time investment. Here I list just a handful of popular user-written Stata packages that can be very helpful when working with large datasets. I've personally used all of these at some point or another and can vouch they can be really helpful: reghdfe (and ivreghdfe , ppmlhdfe ): High-dimensional fixed effects for regression modes. parallel Parallelize code execution. gtools Fast by-able data management and summary statistics (disclamer: I authored this package). ftools : Fast implementation of several Stata commands (e.g. fegen group , fcollapse , fmerge , fisid , flevelsof , fsort ). This package was the inspiration for gtools and while its functions are slower than their gtools counterparts, it retains some benefits: Namely there is no gtools counterpart of fmerge , and its mata API can be very useful.","title":"Other Resources"},{"location":"efficient/other-resources.html#dont-reinvent-the-wheel","text":"Someone may have already solved your problem. There are many user-written packages and programs that are designed to be efficient and may already do what you want to do. While custom tools tailored to your individual situation will typically be faster, it's often the case that the general-purpose tools are fast enough to justify avoiding making the time investment. Here I list just a handful of popular user-written Stata packages that can be very helpful when working with large datasets. I've personally used all of these at some point or another and can vouch they can be really helpful: reghdfe (and ivreghdfe , ppmlhdfe ): High-dimensional fixed effects for regression modes. parallel Parallelize code execution. gtools Fast by-able data management and summary statistics (disclamer: I authored this package). ftools : Fast implementation of several Stata commands (e.g. fegen group , fcollapse , fmerge , fisid , flevelsof , fsort ). This package was the inspiration for gtools and while its functions are slower than their gtools counterparts, it retains some benefits: Namely there is no gtools counterpart of fmerge , and its mata API can be very useful.","title":"Don't reinvent the wheel!"},{"location":"efficient/plan-ahead.html","text":"Planning ahead Sorting and by For example, if you will be doing many operations by group, sorting the data and working on it by group all at once will make each operation much faster. (NB: While gtools functions are also faster on sorted data, of course part of the point of gtools is to obviate the need to do a sort, and the speed gain is often larger without sorting.) set seed 1729 clear set obs 10000000 gen x = rnormal () gen y = rnormal () gen g = mod (_n, 100 ) gen r = runiform () sort r set rmsg on If should be clear that this is inefficient: qui { bys g : gen a = sum (x) bys g : replace a = a[_N] / _N sort r gen b = . bys g : replace b = max (y, b[_n - 1 ]) bys g : replace b = b[_N] sort r gen c = . bys g : replace c = min (y, c[_n - 1 ]) bys g : replace c = c[_N] sort r } However, we might accidentally end up doing a version of this if we're not deliberate about doing similar operations together. A better way to do it would be drop a b c qui { sort g by g : gen a = sum (x) by g : replace a = a[_N] / _N gen b = . by g : replace b = max (y, b[_n - 1 ]) by g : replace b = b[_N] gen c = . by g : replace c = min (y, c[_n - 1 ]) by g : replace c = c[_N] sort r } Of course, gegen is faster in this case if you care about leaving the data in its original state: drop a b c { gegen a = sum (x), by (g) gegen b = max (y), by (g) gegen c = min (y), by (g) } But you should know two things. First, this is an inefficient gtools solution, and we should be using the merge option from gcollapse : drop a b c gcollapse (sum) a=x (max) b=y (min) c=y, by (g) merge Second, if we don't benchmark the sorts , the individual series of by operations in Stata are faster than even this gcollapse statement. The reason is that gcollapse has to group the data internally, whereas the by statement relies on the sort statement for that; so it's not a one-to-one comparison, but if your data will be sorted anyway, you should know that sometimes relying on by can be the fastest solution! Pre-computing variables You should pre-compute variables that will be re-used instead of creating them on the fly. For example: gen byte ind = ... program1 if ind == 1 program2 if ind == 1 gen var = ... for i = 3 / 7 { program `i' var } Very long operations Sometimes a program that takes a long time to run is inevitable: Run overnight or over a break. (So program does not compete for computing time or your own time.) Include checkpoints: Do not write a single function to do all your work. Group tasks into programs, and save your data along the way. Print messages along your program to tell you where you are (can check log while program executes). This program groups execution and has checkpoints and log messages throughout: program part1 display \"part 1, task 1\" * ... display \"part 1, task 2\" * ... end program part2 display \"part 2, task 1\" * ... display \"part 2, task 2\" * ... end part1 save checkpoint1 . dta display \"finished part 1\" part2 save checkpoint2 . dta display \"finished part 2\" We can even be fairly sophisticated about it. The snippet below can scale to multiple parts, for instance: program main syntax , [cached] local nparts = 2 local startat = 1 if \" `cached' \" == \"cached\" { forvalues part = 1 / `nparts' { cap confirm file checkpoint `part' .dta local startat = cond ( `startat' == `part' & _rc == 0 , `part' + 1 , `startat' ) } } forvalues part = 1 / `nparts' { if `startat' <= `part' { part `part' save checkpoint `part' .dta, replace display \"finished part 1\" } else if `startat' == ( `part' + 1 ) { display \"loading cached part `part' results\" use checkpoint `part' .dta, clear } } if `startat' > `nparts' { disp \"Nothing to do with option -cached-; all checkpoints already exist\" } end program part1 display \"part 1, task 1\" * ... display \"part 1, task 2\" * ... end program part2 display \"part 2, task 1\" * ... display \"part 2, task 2\" * ... end main, cached","title":"Planning Ahead"},{"location":"efficient/plan-ahead.html#planning-ahead","text":"","title":"Planning ahead"},{"location":"efficient/plan-ahead.html#sorting-and-by","text":"For example, if you will be doing many operations by group, sorting the data and working on it by group all at once will make each operation much faster. (NB: While gtools functions are also faster on sorted data, of course part of the point of gtools is to obviate the need to do a sort, and the speed gain is often larger without sorting.) set seed 1729 clear set obs 10000000 gen x = rnormal () gen y = rnormal () gen g = mod (_n, 100 ) gen r = runiform () sort r set rmsg on If should be clear that this is inefficient: qui { bys g : gen a = sum (x) bys g : replace a = a[_N] / _N sort r gen b = . bys g : replace b = max (y, b[_n - 1 ]) bys g : replace b = b[_N] sort r gen c = . bys g : replace c = min (y, c[_n - 1 ]) bys g : replace c = c[_N] sort r } However, we might accidentally end up doing a version of this if we're not deliberate about doing similar operations together. A better way to do it would be drop a b c qui { sort g by g : gen a = sum (x) by g : replace a = a[_N] / _N gen b = . by g : replace b = max (y, b[_n - 1 ]) by g : replace b = b[_N] gen c = . by g : replace c = min (y, c[_n - 1 ]) by g : replace c = c[_N] sort r } Of course, gegen is faster in this case if you care about leaving the data in its original state: drop a b c { gegen a = sum (x), by (g) gegen b = max (y), by (g) gegen c = min (y), by (g) } But you should know two things. First, this is an inefficient gtools solution, and we should be using the merge option from gcollapse : drop a b c gcollapse (sum) a=x (max) b=y (min) c=y, by (g) merge Second, if we don't benchmark the sorts , the individual series of by operations in Stata are faster than even this gcollapse statement. The reason is that gcollapse has to group the data internally, whereas the by statement relies on the sort statement for that; so it's not a one-to-one comparison, but if your data will be sorted anyway, you should know that sometimes relying on by can be the fastest solution!","title":"Sorting and by"},{"location":"efficient/plan-ahead.html#pre-computing-variables","text":"You should pre-compute variables that will be re-used instead of creating them on the fly. For example: gen byte ind = ... program1 if ind == 1 program2 if ind == 1 gen var = ... for i = 3 / 7 { program `i' var }","title":"Pre-computing variables"},{"location":"efficient/plan-ahead.html#very-long-operations","text":"Sometimes a program that takes a long time to run is inevitable: Run overnight or over a break. (So program does not compete for computing time or your own time.) Include checkpoints: Do not write a single function to do all your work. Group tasks into programs, and save your data along the way. Print messages along your program to tell you where you are (can check log while program executes). This program groups execution and has checkpoints and log messages throughout: program part1 display \"part 1, task 1\" * ... display \"part 1, task 2\" * ... end program part2 display \"part 2, task 1\" * ... display \"part 2, task 2\" * ... end part1 save checkpoint1 . dta display \"finished part 1\" part2 save checkpoint2 . dta display \"finished part 2\" We can even be fairly sophisticated about it. The snippet below can scale to multiple parts, for instance: program main syntax , [cached] local nparts = 2 local startat = 1 if \" `cached' \" == \"cached\" { forvalues part = 1 / `nparts' { cap confirm file checkpoint `part' .dta local startat = cond ( `startat' == `part' & _rc == 0 , `part' + 1 , `startat' ) } } forvalues part = 1 / `nparts' { if `startat' <= `part' { part `part' save checkpoint `part' .dta, replace display \"finished part 1\" } else if `startat' == ( `part' + 1 ) { display \"loading cached part `part' results\" use checkpoint `part' .dta, clear } } if `startat' > `nparts' { disp \"Nothing to do with option -cached-; all checkpoints already exist\" } end program part1 display \"part 1, task 1\" * ... display \"part 1, task 2\" * ... end program part2 display \"part 2, task 1\" * ... display \"part 2, task 2\" * ... end main, cached","title":"Very long operations"},{"location":"efficient/right-tool.html","text":"Use the best tool for the job No program is the best at everything! Stata: Easy to use, but can be slow in places. C: Extremely fast (e.g. underpins gtools) but hard to learn. Mata: Stata's embedded matrix language, can be used to speed-up many simple tasks. Before even dreaming of learning something like C, you should look into mata if you're using Stata! Frames: Available from Stata 16, allows the user to have multiple datasets in memory. Python: Built-in interface from Stata 16; allows direct interaction with Python.","title":"The Right Tool"},{"location":"efficient/right-tool.html#use-the-best-tool-for-the-job","text":"No program is the best at everything! Stata: Easy to use, but can be slow in places. C: Extremely fast (e.g. underpins gtools) but hard to learn. Mata: Stata's embedded matrix language, can be used to speed-up many simple tasks. Before even dreaming of learning something like C, you should look into mata if you're using Stata! Frames: Available from Stata 16, allows the user to have multiple datasets in memory. Python: Built-in interface from Stata 16; allows direct interaction with Python.","title":"Use the best tool for the job"},{"location":"efficient/trade-off.html","text":"Coding fast vs fast code Code that executes quickly is part of coding efficiently, but there's a trade-off: Writing fast code is typically time-consuming. The comic ( xkcd 1205 ) shows the maximum amount of time you can spend optimizng a task before it becomes inefficient by how much time you are saving and how often the task is run, over a 5-year horizon. If you do a task once a year and you will only save one minute, then the most time you should spend on making it faster is 5 minutes. But if you do it several times a day, say 5/day, then it might make sense to spend up to 6 days making it faster. In practical coding situations the trade-off is seldom so clear-cut, but the exact idea does apply. It makes little sense to spend a long time optimizing code you will only run a handful of times. By contrast, if you expect you will have to run it often then making it run faster might well be worth the investment.","title":"Trade-Off"},{"location":"efficient/trade-off.html#coding-fast-vs-fast-code","text":"Code that executes quickly is part of coding efficiently, but there's a trade-off: Writing fast code is typically time-consuming. The comic ( xkcd 1205 ) shows the maximum amount of time you can spend optimizng a task before it becomes inefficient by how much time you are saving and how often the task is run, over a 5-year horizon. If you do a task once a year and you will only save one minute, then the most time you should spend on making it faster is 5 minutes. But if you do it several times a day, say 5/day, then it might make sense to spend up to 6 days making it faster. In practical coding situations the trade-off is seldom so clear-cut, but the exact idea does apply. It makes little sense to spend a long time optimizing code you will only run a handful of times. By contrast, if you expect you will have to run it often then making it run faster might well be worth the investment.","title":"Coding fast vs fast code"},{"location":"efficient/algorithms/bootstrap.html","text":"Algorithm Showdown: Bootstrap The typical bootstrap involves drawing a random sample of the data and re-computing your summary statistic. This can be a slow process (even Stata's built-in bootstrap can be slow). While in many situations this is the only way to bootstrap your data, the Bayesian bootstrap (Rubin, 1981) is much more efficient (computationally) and works in most common scenarios: Draw iid Dirichlet weights (one for each observation). Recompute statistic, weighted. That's it! (Added bonus: Since all observations are kept, just re-weighted, it's also more stable for e.g. regressions, as no variables can unexpectedly drop due to collinearity.) Let's look at a concrete example; the built-in command takes about a minute here: clear set seed 1 set rmsg on set obs 1000000 gen x = rnormal () * 2 gen y = x + rnormal () * 10 bootstrap , reps( 100 ): reg y x ------------------------------------- | Observed Bootstrap y | coefficient std. err . -------------+----------------------- ... x | . 9925311 . 0050175 _cons | - . 0151513 . 0087632 ------------------------------------- The Bayesian bootstrap runs in 20s (3x as fast)! mata b = J ( 100 , 2 , .) tempvar weight qui gen `weight' = . qui forvalues b = 1 / 100 { replace `weight' = rgamma ( 1 , 1 ) sum `weight' , meanonly replace `weight' = `weight' / r (sum) reg y x [pw = `weight' ] mata b[ `b' , .] = st_matrix( \"e(b)\" ) } mata diagonal( sqrt (variance(b)))' 1 2 +-----------------------------+ 1 | . 0048730429 . 0093159813 | +-----------------------------+","title":"Bootstrap"},{"location":"efficient/algorithms/bootstrap.html#algorithm-showdown-bootstrap","text":"The typical bootstrap involves drawing a random sample of the data and re-computing your summary statistic. This can be a slow process (even Stata's built-in bootstrap can be slow). While in many situations this is the only way to bootstrap your data, the Bayesian bootstrap (Rubin, 1981) is much more efficient (computationally) and works in most common scenarios: Draw iid Dirichlet weights (one for each observation). Recompute statistic, weighted. That's it! (Added bonus: Since all observations are kept, just re-weighted, it's also more stable for e.g. regressions, as no variables can unexpectedly drop due to collinearity.) Let's look at a concrete example; the built-in command takes about a minute here: clear set seed 1 set rmsg on set obs 1000000 gen x = rnormal () * 2 gen y = x + rnormal () * 10 bootstrap , reps( 100 ): reg y x ------------------------------------- | Observed Bootstrap y | coefficient std. err . -------------+----------------------- ... x | . 9925311 . 0050175 _cons | - . 0151513 . 0087632 ------------------------------------- The Bayesian bootstrap runs in 20s (3x as fast)! mata b = J ( 100 , 2 , .) tempvar weight qui gen `weight' = . qui forvalues b = 1 / 100 { replace `weight' = rgamma ( 1 , 1 ) sum `weight' , meanonly replace `weight' = `weight' / r (sum) reg y x [pw = `weight' ] mata b[ `b' , .] = st_matrix( \"e(b)\" ) } mata diagonal( sqrt (variance(b)))' 1 2 +-----------------------------+ 1 | . 0048730429 . 0093159813 | +-----------------------------+","title":"Algorithm Showdown: Bootstrap"},{"location":"efficient/algorithms/introduction.html","text":"Improve your Algorithms A parallelized and extremely efficient loop is slower than the equivalent vector operation. The fastest regression with a full set of fixed-effect indicators is slower than reghdfe . Code tailored to your specific task is faster than general-purpose code. A simple example I think most people should find intuitive is computing a leave-one-out mean. One definition is \\[ \\bar{x}_{-i} = \\dfrac{\\sum_{j \\ne i} x_j}{\\sum_{j \\ne i} 1} \\] And the corresponding code is given by clear set rmsg on set obs 10000 gen x = runiform () gen x_loo = . forvalues i = 1 / `=_N' { qui sum x if _n != `i' , meanonly qui replace x_loo = r (mean) in `i' } With only 10,000 observations this already takes a few seconds. Another definition, which is one I think most people will have used, is $$ \\bar{x}_{-i} = \\dfrac{(\\sum_j x_j) - x_i}{(\\sum_j 1) - 1} $$ qui sum x, meanonly gen x_loo2 = ( r (sum) - x) / ( r (N) - 1 ) assert x_loo == x_loo2 clear set obs 10000000 gen x = runiform () qui sum x, meanonly gen x_loo2 = ( r (sum) - x) / ( r (N) - 1 ) Even with 10M observations, this method gives the answer in a fraction of a second. In this case, it truly would not have mattered how fast you make your loop: The faster algorithm will win every time.","title":"Introduction"},{"location":"efficient/algorithms/introduction.html#improve-your-algorithms","text":"A parallelized and extremely efficient loop is slower than the equivalent vector operation. The fastest regression with a full set of fixed-effect indicators is slower than reghdfe . Code tailored to your specific task is faster than general-purpose code. A simple example I think most people should find intuitive is computing a leave-one-out mean. One definition is \\[ \\bar{x}_{-i} = \\dfrac{\\sum_{j \\ne i} x_j}{\\sum_{j \\ne i} 1} \\] And the corresponding code is given by clear set rmsg on set obs 10000 gen x = runiform () gen x_loo = . forvalues i = 1 / `=_N' { qui sum x if _n != `i' , meanonly qui replace x_loo = r (mean) in `i' } With only 10,000 observations this already takes a few seconds. Another definition, which is one I think most people will have used, is $$ \\bar{x}_{-i} = \\dfrac{(\\sum_j x_j) - x_i}{(\\sum_j 1) - 1} $$ qui sum x, meanonly gen x_loo2 = ( r (sum) - x) / ( r (N) - 1 ) assert x_loo == x_loo2 clear set obs 10000000 gen x = runiform () qui sum x, meanonly gen x_loo2 = ( r (sum) - x) / ( r (N) - 1 ) Even with 10M observations, this method gives the answer in a fraction of a second. In this case, it truly would not have mattered how fast you make your loop: The faster algorithm will win every time.","title":"Improve your Algorithms"},{"location":"efficient/algorithms/merge.html","text":"Algorithm Showdown: Merge Often it is possible to merge on single, integer IDs (vs arbitrary sets of variables). merge 1 : 1 id using data . dta // vs merge 1 : 1 var_double var_str8 var_long // ... Sorting and matching on a single variable is much faster than sorting and matching many variables of different types, regardless of how efficient you make the sort or the merge itself! Even merging on multiple integers is faster than merging on multiple mixed-type variables. Anecdote: Recasting Data Types I was working with insurance data on 80M individuals, and their ID was a 21-character variable. The first thing I did was map them into an integer ID numbered 1 through 80M and then I worked with that 4-byte identifier for the rest of the analysis. I knew every merge and group operation would be much faster! In that project I did the same type of re-mapping for basically every variable I could (e.g. categorical demographic data). Recently someone asked me if I could help speed up their merge. All I suggested was that they recast each of their variables as their smallest type (all their numbers were doubles, but I suggested they recast dates as 2-byte integers, some categories as 1-byte integers, IDs as 4 or 8-byte integers, etc.) Their merge was suddenly 2-3 times faster just with that change.","title":"Merge"},{"location":"efficient/algorithms/merge.html#algorithm-showdown-merge","text":"Often it is possible to merge on single, integer IDs (vs arbitrary sets of variables). merge 1 : 1 id using data . dta // vs merge 1 : 1 var_double var_str8 var_long // ... Sorting and matching on a single variable is much faster than sorting and matching many variables of different types, regardless of how efficient you make the sort or the merge itself! Even merging on multiple integers is faster than merging on multiple mixed-type variables. Anecdote: Recasting Data Types I was working with insurance data on 80M individuals, and their ID was a 21-character variable. The first thing I did was map them into an integer ID numbered 1 through 80M and then I worked with that 4-byte identifier for the rest of the analysis. I knew every merge and group operation would be much faster! In that project I did the same type of re-mapping for basically every variable I could (e.g. categorical demographic data). Recently someone asked me if I could help speed up their merge. All I suggested was that they recast each of their variables as their smallest type (all their numbers were doubles, but I suggested they recast dates as 2-byte integers, some categories as 1-byte integers, IDs as 4 or 8-byte integers, etc.) Their merge was suddenly 2-3 times faster just with that change.","title":"Algorithm Showdown: Merge"},{"location":"memory/minimize.html","text":"Minimize Memory Use Use Small Variable Types Stata has multiple variable types. The default is float , and you can see here why storing everything as a float can lead to problems. However, storing everything as a double is excessive in most situations. In general, store variables in the smallest sensible type for the data they have: double: Largest number type (8 bytes). Integers in -/+ 2 53 , numbers in -/+ 8.988e307. float: Stata default type (4 bytes). Integers in -/+ 2 24 , numbers in -/+ 1.701e38. long: Largest integer type (4 bytes). Integers from -2 31 +1 to 2 31 -28. int: Small integer type (2 bytes). Integers from -2 15 +1 to 2 15 -28. byte: Smallest integer type (1 bytes). Integers from -2 7 +1 to 2 7 -28. str#: String data from str1 to str2045 (one byte per character; e.g. str10 is 10 bytes). strL: Arbitrary data (in general, binary data, but includes strings of variable width) up to 2e9 bytes per row (has a 80 byte overhead, so storage is 80 bytes per row plus the space taken up by the data it stores). In general you'll use float for reals (by default), long for integers, and str# for strings. However, there are plenty of variables that are bounded and you can save memory by using byte (e.g. indicators, sparse categorical variables) or int (e.g. dates, day of the year). It's often hard to know the best type to use for all your variables (and sometimes, when coding quickly, you can simply forger to use anything other than the default for numbers and long for integers). compress is a Stata command that recasts all your variables as the smallest safe type. Always compress data It is always (almost) a good idea, and always safe, to use compress on your data. The only scenario you wouldn't do that is if you cannot spare the computing time (compress can be very cpu-intensive in some cases). In this case, compression refers specifically to variable recasting to save memory and is a completely lossless operation: The exact same data is retained, but wastefully allocated space is freed. (In many other computing settings, compression refers to encoding information in fewer bytes to save space; in other words, the data is altered so the encoded version takes up less space, and the original copy can be recovered by uncompressing the data. Stata's compress does not refer to this.) Anecdote: Poorly typed data I once received a Stata file over 150GiB in size. While it had close to 160M rows, it only had 6 variables. I noticed 4 were string variables of type str256, but some quick exploration revealed they were really str8. I looped through the data in chunks, using compress on each chunk, and then appended the chunks at the end. (By chunk I mean a range of rows, which is efficient in Stata as data is stored by row; I did it this way to avoid reading too much data into memory at a time.) The final data only took up around 5GiB. Make sure numbers are numbers It almost always takes less memory to store numbers as numbers than as strings, but very often data that should be numeric is stored as string (this can happen often when, for example, importing external data). destring varlist, replace is a safe way of making sure you don't have any numbers hidden in strings. As compress , there are situations where it can be cpu-intensive, but if your strings contain non-numeric data it will not force them to become numbers. The only (default) caveat as far as I know is that missing values are interpreted, even though they're not numbers per se (so . , .a , .b and so on are read as a missing values). Sometimes you need to be a bit more proactive: Say you have hourly wages, rounded, for a group of individuals (\u20ac20, \u20ac30, etc.) truncated above by \u20ac1,000. If you import this data into Stata, it would be stored as str6, but it can be stored as a 2-byte integer. In straightforward cases like this, you can just use the ignore option in destring , but in general you may need to clean up your strings before converting them to numbers: . clear . qui set obs 3 . qui gen str6 wages = \"\u20ac20\" in 1 . qui replace wages = \"\u20ac30\" in 2 . qui replace wages = \"\u20ac1,000\" in 3 . destring wages, replace wages: contains nonnumeric characters; no replace . destring wages, replace ignore( \"\u20ac,\" ) wages: characters \u20ac , removed; replaced as int Why can memory use blow up anyway? Most ancillary objects take little memory (with exceptions), but many programs make copies of the data internally. It's often not possible to modify the data in place. Even if possible, it's often more efficient to make copies. Examples: gtools has to make a copy of all the variables that will be used, in addition to all internal objects. It can be a memory hog! parallel makes copies of all ancillary objects and starts sepparate instances of Stata, each with its own resource use. For \"normal\" use cases, each instance would only have a cut of the data, so the memory overhead should be minimal. However, depending on your operation, memory use can increase by up to the number of cores requested. mata , frame , and python can all make large objects independent of what is stored in the current dataset. Short demo with mata, frames, and Python log using memtest . log, replace * Define function to check this Stata instance's current memory python: import os, psutil def checK_memory(): print (f \"{psutil.Process(os.getpid()).memory_info().rss / 1024**2:,.1f} MiB in use\" ) end * Check space taken by integer ID python: checK_memory() set obs `:disp %12.0f 1e8' gen `c(obs_t)' id = _n python: checK_memory() * Total space after copying to mata mata id = st_data(., \"id\" ) python: checK_memory() * Total space after copying to second frame frame put id, into(id) python: checK_memory() * Total space after copying to Python python: from sfi import Data id = Data . get (var= \"id\" ) checK_memory() end * Check space after droping each object drop id python: checK_memory() mata mata drop id python: checK_memory() frame drop id python: checK_memory() python: id=None python: checK_memory() * Check objects were dropped frame dir mata mata desc python: [k for k in globals().keys() if not k . startswith('_')] log close _all These are the statements printed by each checK_memory invocation, in Stata 17/MP: 38.7 MiB: Baseline memory use. 610.8 MiB: Minimum space taken up by 4-byte variable with 100M rows is 381.5 MiB; you can see Stata has an additional 190MiB overhead; this overhead will vary depending on your dataset. 1,374.3 MiB: Mata stores numbers as 8-byte doubles; minimum space taken up with 100M rows is 762.9 MiB; you can see there was virtually no overhead in this case. 1,946.8 MiB: This effectively makes a copy of the current data frame; it's not too surprising that the additional memory is almost identical here than vs the first step. 5,773.5 MiB: Python should store a copy as a double similar to mata; however, we can see python's overhead is nearly 3GiB in this case. I am not entirely sure why this happens, but you should certainly be quite mindful of this when interfacing with Python from Stata. It seems there's intermediate objects that are created in a way that is not always memory-efficient. 5,773.5 MiB: Here I dropped the id variable but memory requirements stayed the same. I'm not entirely sure why this is, but this doesn't happen if you don't make all the copies of the ID variables we made (e.g. try dropping id right after creation and then run python: check_memory() ). My guess is that Stata is trying to be smart about your memory requirements and is concluding that you might need it in the future. 5,012.7 MiB: Dropped mata object id . 4,438.5 MiB: Dropped frame id . 613.2 MiB: Dropped Python object; again, the drop is disproportionate to what we might expect, shoring python has a hefty memory overhead from within Stata.","title":"Minimize Memory Use"},{"location":"memory/minimize.html#minimize-memory-use","text":"","title":"Minimize Memory Use"},{"location":"memory/minimize.html#use-small-variable-types","text":"Stata has multiple variable types. The default is float , and you can see here why storing everything as a float can lead to problems. However, storing everything as a double is excessive in most situations. In general, store variables in the smallest sensible type for the data they have: double: Largest number type (8 bytes). Integers in -/+ 2 53 , numbers in -/+ 8.988e307. float: Stata default type (4 bytes). Integers in -/+ 2 24 , numbers in -/+ 1.701e38. long: Largest integer type (4 bytes). Integers from -2 31 +1 to 2 31 -28. int: Small integer type (2 bytes). Integers from -2 15 +1 to 2 15 -28. byte: Smallest integer type (1 bytes). Integers from -2 7 +1 to 2 7 -28. str#: String data from str1 to str2045 (one byte per character; e.g. str10 is 10 bytes). strL: Arbitrary data (in general, binary data, but includes strings of variable width) up to 2e9 bytes per row (has a 80 byte overhead, so storage is 80 bytes per row plus the space taken up by the data it stores). In general you'll use float for reals (by default), long for integers, and str# for strings. However, there are plenty of variables that are bounded and you can save memory by using byte (e.g. indicators, sparse categorical variables) or int (e.g. dates, day of the year). It's often hard to know the best type to use for all your variables (and sometimes, when coding quickly, you can simply forger to use anything other than the default for numbers and long for integers). compress is a Stata command that recasts all your variables as the smallest safe type. Always compress data It is always (almost) a good idea, and always safe, to use compress on your data. The only scenario you wouldn't do that is if you cannot spare the computing time (compress can be very cpu-intensive in some cases). In this case, compression refers specifically to variable recasting to save memory and is a completely lossless operation: The exact same data is retained, but wastefully allocated space is freed. (In many other computing settings, compression refers to encoding information in fewer bytes to save space; in other words, the data is altered so the encoded version takes up less space, and the original copy can be recovered by uncompressing the data. Stata's compress does not refer to this.) Anecdote: Poorly typed data I once received a Stata file over 150GiB in size. While it had close to 160M rows, it only had 6 variables. I noticed 4 were string variables of type str256, but some quick exploration revealed they were really str8. I looped through the data in chunks, using compress on each chunk, and then appended the chunks at the end. (By chunk I mean a range of rows, which is efficient in Stata as data is stored by row; I did it this way to avoid reading too much data into memory at a time.) The final data only took up around 5GiB.","title":"Use Small Variable Types"},{"location":"memory/minimize.html#make-sure-numbers-are-numbers","text":"It almost always takes less memory to store numbers as numbers than as strings, but very often data that should be numeric is stored as string (this can happen often when, for example, importing external data). destring varlist, replace is a safe way of making sure you don't have any numbers hidden in strings. As compress , there are situations where it can be cpu-intensive, but if your strings contain non-numeric data it will not force them to become numbers. The only (default) caveat as far as I know is that missing values are interpreted, even though they're not numbers per se (so . , .a , .b and so on are read as a missing values). Sometimes you need to be a bit more proactive: Say you have hourly wages, rounded, for a group of individuals (\u20ac20, \u20ac30, etc.) truncated above by \u20ac1,000. If you import this data into Stata, it would be stored as str6, but it can be stored as a 2-byte integer. In straightforward cases like this, you can just use the ignore option in destring , but in general you may need to clean up your strings before converting them to numbers: . clear . qui set obs 3 . qui gen str6 wages = \"\u20ac20\" in 1 . qui replace wages = \"\u20ac30\" in 2 . qui replace wages = \"\u20ac1,000\" in 3 . destring wages, replace wages: contains nonnumeric characters; no replace . destring wages, replace ignore( \"\u20ac,\" ) wages: characters \u20ac , removed; replaced as int","title":"Make sure numbers are numbers"},{"location":"memory/minimize.html#why-can-memory-use-blow-up-anyway","text":"Most ancillary objects take little memory (with exceptions), but many programs make copies of the data internally. It's often not possible to modify the data in place. Even if possible, it's often more efficient to make copies. Examples: gtools has to make a copy of all the variables that will be used, in addition to all internal objects. It can be a memory hog! parallel makes copies of all ancillary objects and starts sepparate instances of Stata, each with its own resource use. For \"normal\" use cases, each instance would only have a cut of the data, so the memory overhead should be minimal. However, depending on your operation, memory use can increase by up to the number of cores requested. mata , frame , and python can all make large objects independent of what is stored in the current dataset.","title":"Why can memory use blow up anyway?"},{"location":"memory/minimize.html#short-demo-with-mata-frames-and-python","text":"log using memtest . log, replace * Define function to check this Stata instance's current memory python: import os, psutil def checK_memory(): print (f \"{psutil.Process(os.getpid()).memory_info().rss / 1024**2:,.1f} MiB in use\" ) end * Check space taken by integer ID python: checK_memory() set obs `:disp %12.0f 1e8' gen `c(obs_t)' id = _n python: checK_memory() * Total space after copying to mata mata id = st_data(., \"id\" ) python: checK_memory() * Total space after copying to second frame frame put id, into(id) python: checK_memory() * Total space after copying to Python python: from sfi import Data id = Data . get (var= \"id\" ) checK_memory() end * Check space after droping each object drop id python: checK_memory() mata mata drop id python: checK_memory() frame drop id python: checK_memory() python: id=None python: checK_memory() * Check objects were dropped frame dir mata mata desc python: [k for k in globals().keys() if not k . startswith('_')] log close _all These are the statements printed by each checK_memory invocation, in Stata 17/MP: 38.7 MiB: Baseline memory use. 610.8 MiB: Minimum space taken up by 4-byte variable with 100M rows is 381.5 MiB; you can see Stata has an additional 190MiB overhead; this overhead will vary depending on your dataset. 1,374.3 MiB: Mata stores numbers as 8-byte doubles; minimum space taken up with 100M rows is 762.9 MiB; you can see there was virtually no overhead in this case. 1,946.8 MiB: This effectively makes a copy of the current data frame; it's not too surprising that the additional memory is almost identical here than vs the first step. 5,773.5 MiB: Python should store a copy as a double similar to mata; however, we can see python's overhead is nearly 3GiB in this case. I am not entirely sure why this happens, but you should certainly be quite mindful of this when interfacing with Python from Stata. It seems there's intermediate objects that are created in a way that is not always memory-efficient. 5,773.5 MiB: Here I dropped the id variable but memory requirements stayed the same. I'm not entirely sure why this is, but this doesn't happen if you don't make all the copies of the ID variables we made (e.g. try dropping id right after creation and then run python: check_memory() ). My guess is that Stata is trying to be smart about your memory requirements and is concluding that you might need it in the future. 5,012.7 MiB: Dropped mata object id . 4,438.5 MiB: Dropped frame id . 613.2 MiB: Dropped Python object; again, the drop is disproportionate to what we might expect, shoring python has a hefty memory overhead from within Stata.","title":"Short demo with mata, frames, and Python"},{"location":"memory/swapping.html","text":"Caution! Swapping What is swapping? RAM (memory) is both limited and essential for an operating system. By and large an OS will take one of two measures to avoid running out of memory and ensure it can continue to function: Free up memory by Killing/stopping programs. Move programs' data out of memory. Swap is a dedicated space in a computer's storage that acts as a backup for situations where the OS needs to move data out of memory. From an OS' point of view, this option is much preferred: Programs can continue to run if their data is moved instead of deleted, and the memory they were using can be reallocated. So what's the catch? Well, there's a reason programs use memory: It's orders of magnitude faster than using storage (even compared to more modern solutions like SSDs). In many cases, of course, you want program to keep running, even if they are doing do much more slowly. For analysis on big data, however, unexpectedly swapping can grind execution to a halt. There are certainly programs optimized to analyze data on disk, but in my experience if a program was written to analyze data in memory and it suddenly finds portions of its data in swap, it might simply not finish in any reasonable time frame. How to avoid it? Minimize your program's memory footprint. Get more memory! But easier said than done. Keep only the essential variables in memory. Chunk your program execution. Sometimes high memory use is inevitable, but before looking for a solution that processes data on disk, there is one more thing to consider: Can you process your data separately for groups of observations (i.e. by row)? Stata stores data in row order, so this can be efficient both in terms of memory and speed. Anecdote: Process data in chunks I once had health insurance enrollment data on 80M individuals. For each person I had an ID and five variables for each month over the span of 10 years. The data had hundreds of variables and I wanted to reshape it long and collapse to annual enrollment. If you do some quick math, you'll realize the reshape would have to allocate billions of observations, which presented an issue from the POV of memory use. At the time I had to run this on a shared server, and I was concerned that even if this fit in memory, when anyone else ran a program with even modest memory requirements, my reshape would start to swap and then might never finish. The solution I cape up with was to process the data in chunks: At each iteration of the loop, I read in 8M individuals, reshaped the data, and collapsed to annual enrollment. This was possible each individual's enrollment was independent from the other, and suddenly memory requirements dropped by an order of magnitude.","title":"Caution! Swapping"},{"location":"memory/swapping.html#caution-swapping","text":"","title":"Caution! Swapping"},{"location":"memory/swapping.html#what-is-swapping","text":"RAM (memory) is both limited and essential for an operating system. By and large an OS will take one of two measures to avoid running out of memory and ensure it can continue to function: Free up memory by Killing/stopping programs. Move programs' data out of memory. Swap is a dedicated space in a computer's storage that acts as a backup for situations where the OS needs to move data out of memory. From an OS' point of view, this option is much preferred: Programs can continue to run if their data is moved instead of deleted, and the memory they were using can be reallocated. So what's the catch? Well, there's a reason programs use memory: It's orders of magnitude faster than using storage (even compared to more modern solutions like SSDs). In many cases, of course, you want program to keep running, even if they are doing do much more slowly. For analysis on big data, however, unexpectedly swapping can grind execution to a halt. There are certainly programs optimized to analyze data on disk, but in my experience if a program was written to analyze data in memory and it suddenly finds portions of its data in swap, it might simply not finish in any reasonable time frame.","title":"What is swapping?"},{"location":"memory/swapping.html#how-to-avoid-it","text":"Minimize your program's memory footprint. Get more memory! But easier said than done. Keep only the essential variables in memory. Chunk your program execution. Sometimes high memory use is inevitable, but before looking for a solution that processes data on disk, there is one more thing to consider: Can you process your data separately for groups of observations (i.e. by row)? Stata stores data in row order, so this can be efficient both in terms of memory and speed. Anecdote: Process data in chunks I once had health insurance enrollment data on 80M individuals. For each person I had an ID and five variables for each month over the span of 10 years. The data had hundreds of variables and I wanted to reshape it long and collapse to annual enrollment. If you do some quick math, you'll realize the reshape would have to allocate billions of observations, which presented an issue from the POV of memory use. At the time I had to run this on a shared server, and I was concerned that even if this fit in memory, when anyone else ran a program with even modest memory requirements, my reshape would start to swap and then might never finish. The solution I cape up with was to process the data in chunks: At each iteration of the loop, I read in 8M individuals, reshaped the data, and collapsed to annual enrollment. This was possible each individual's enrollment was independent from the other, and suddenly memory requirements dropped by an order of magnitude.","title":"How to avoid it?"}]}